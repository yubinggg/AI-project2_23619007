# ============================================
# ë°ì´í„° ì¤€ë¹„ â†’ í•™ìŠµ â†’ í‰ê°€(ROC) â†’ Grad-CAM(Fake-only) + BLIP ê·¼ê±°
# - ImageFolder ë¼ë²¨ ë§¤í•‘ ì•ˆì „í™”(fake=1, real=0)
# - AMP ì¼ê´€ ì ìš©, Grad-CAMì€ target=fake(1) ê³ ì •
# - í‰ê°€ ë³€í™˜ Resize((224,224))
# - íŒ¨ë„ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ 'ì•„ë˜'ì— ë°°ì¹˜
# ============================================

!pip install -q opencv-python-headless==4.10.0.84 \
               transformers==4.44.2 accelerate==0.34.2 timm==1.0.9 \
               scikit-learn==1.5.1 tqdm==4.66.4

# -------------------------
# 0) ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
# -------------------------
import os, glob, json, csv, math, shutil, random
from pathlib import Path
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Subset
import torchvision.transforms.functional as TF

import cv2

from sklearn.metrics import roc_curve, auc

from google.colab import drive
drive.mount('/content/drive')

# -------------------------
# 1) ê²½ë¡œ ë° ì˜µì…˜ ì„¤ì •
# -------------------------
USE_LOCAL_COPY = False       # Driveì— ìˆëŠ” 224 ë°ì´í„° ë°”ë¡œ ì‚¬ìš©
MAX_WORKERS    = 4
TARGET_SIDE    = 224

# ì „ì²´ ë°ì´í„° ì‚¬ìš©
FRACTION_TRAIN = 1.0
FRACTION_VAL   = 1.0
FRACTION_TEST  = 1.0

EPOCHS   = 20
LR       = 1e-3
WD       = 1e-4
patience = 5  # early stop

ROOT_DRIVE = "/content/drive/MyDrive"

# ğŸ”¹ multi-style ë¦¬ì‚¬ì´ì¦ˆ ë°ì´í„° ê²½ë¡œ
SRC_DRIVE  = f"{ROOT_DRIVE}/celeba_hq_binary_224_multi"       # 224x224 ë¦¬ì‚¬ì´ì¦ˆ ì™„ë£Œëœ ì´ë¯¸ì§€
DST_DRIVE  = SRC_DRIVE                                        # ë³„ë„ ë¦¬ì‚¬ì´ì¦ˆ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
META_DRIVE = f"{ROOT_DRIVE}/celeba_hq_binary_224_multi_meta"  # ë©”íƒ€/ëª¨ë¸ ì €ì¥

# ë¡œì»¬ ë³µì‚¬ ê²½ë¡œ(ì›í•˜ë©´ ë‚˜ì¤‘ì— USE_LOCAL_COPY=Trueë¡œ ë³€ê²½í•´ì„œ ì‚¬ìš©)
SRC_LOCAL = "/content/celeba_hq_binary_224_multi"
DST_LOCAL = SRC_LOCAL
META_LOCAL= META_DRIVE

# ì´ë²ˆ ì„¤ì •ì—ì„œëŠ” USE_LOCAL_COPY=False ì´ë¯€ë¡œ Drive ê²½ë¡œ ì§ì ‘ ì‚¬ìš©
SRC  = SRC_DRIVE
DST  = DST_DRIVE
META = META_DRIVE

# train/val/test, real/fake êµ¬ì¡°ê°€ ì˜ ìˆëŠ”ì§€ í™•ì¸
for sp in ["train","val","test"]:
    for lb in ["real","fake"]:
        assert os.path.isdir(f"{SRC_DRIVE}/{sp}/{lb}"), f"[ERR] missing {SRC_DRIVE}/{sp}/{lb}"

Path(META).mkdir(parents=True, exist_ok=True)
random.seed(42)

print("[OK] working paths:\n SRC:", SRC, "\n DST:", DST, "\n META:", META)

# -------------------------
# 2) Drive â†’ Colab ë™ê¸°í™” (ì´ë²ˆì—” ìŠ¤í‚µ)
# -------------------------
if USE_LOCAL_COPY:
    # í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— Trueë¡œ ë°”ê¿”ì„œ ë¡œì»¬ ë³µì‚¬ ì‚¬ìš© ê°€ëŠ¥
    for sp in ["train","val","test"]:
        for lb in ["real","fake"]:
            d_drive = f"{SRC_DRIVE}/{sp}/{lb}"
            d_local = f"{SRC_LOCAL}/{sp}/{lb}"
            Path(d_local).mkdir(parents=True, exist_ok=True)
            local_names = {os.path.basename(p) for p in glob.glob(f"{d_local}/*")}
            new_files = [p for p in glob.glob(f"{d_drive}/*") if os.path.basename(p) not in local_names]
            for p in new_files:
                shutil.copy2(p, os.path.join(d_local, os.path.basename(p)))
    SRC = SRC_LOCAL
    DST = DST_LOCAL
    print("[INFO] Copied data to local disk.")
else:
    print("[INFO] Using Drive 224 ë°ì´í„° ì§ì ‘ ì‚¬ìš© (ë³µì‚¬ ì—†ìŒ).")

print("[OK] ìµœì¢… ë°ì´í„° ê²½ë¡œ:\n SRC:", SRC, "\n DST:", DST)

# -------------------------
# 3) 224 ë¦¬ì‚¬ì´ì¦ˆ ë‹¨ê³„ (ì´ë¯¸ ë¦¬ì‚¬ì´ì¦ˆ ì™„ë£Œë¼ ìŠ¤í‚µ)
# -------------------------
print("[INFO] ì´ë¯¸ 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆëœ ë°ì´í„° ì‚¬ìš© â†’ ë¦¬ì‚¬ì´ì¦ˆ ë‹¨ê³„ ìŠ¤í‚µ.")

# -------------------------
# 4) ì •ê·œí™” íŒŒë¼ë¯¸í„° (ê°„ë‹¨ ë²„ì „: ImageNet mean/std ìƒìˆ˜ ì‚¬ìš©)
# -------------------------
# ResNetì´ ì‚¬ì „ í•™ìŠµëœ ImageNet í†µê³„
DMEAN = [0.485, 0.456, 0.406]
DSTD  = [0.229, 0.224, 0.225]

print("[OK] Using fixed ImageNet mean/std:", DMEAN, DSTD)

# -------------------------
# 5) ë°ì´í„°ì…‹/ë¡œë” êµ¬ì„± (ë¼ë²¨ ë§¤í•‘ ì•ˆì „í™”)
# -------------------------
train_tf = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.ColorJitter(0.2,0.2,0.2,0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=DMEAN, std=DSTD)
])
eval_tf = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=DMEAN, std=DSTD)
])

_tmp = datasets.ImageFolder(f"{DST}/train")
CLASSES = _tmp.classes  # ë³´í†µ ['fake','real'] ë˜ëŠ” ['real','fake']
print("[INFO] CLASSES =", CLASSES)

def remap_target(y):
    # ImageFolder í´ë˜ìŠ¤ ì´ë¦„ ê¸°ì¤€ìœ¼ë¡œ fake=1, real=0 ê°•ì œ
    return 1 if CLASSES[y] == 'fake' else 0

train_full = datasets.ImageFolder(f"{DST}/train", transform=train_tf, target_transform=remap_target)
val_full   = datasets.ImageFolder(f"{DST}/val",   transform=eval_tf,  target_transform=remap_target)
test_full  = datasets.ImageFolder(f"{DST}/test",  transform=eval_tf,  target_transform=remap_target)

def subset_by_fraction(dataset, frac, seed=42):
    if frac >= 1.0:
        return dataset
    n_total = len(dataset)
    n_keep  = max(1, int(n_total * frac))
    g = torch.Generator().manual_seed(seed)
    idx = torch.randperm(n_total, generator=g)[:n_keep].tolist()
    return Subset(dataset, idx)

train_ds = subset_by_fraction(train_full, FRACTION_TRAIN)
val_ds   = subset_by_fraction(val_full,   FRACTION_VAL)
test_ds  = subset_by_fraction(test_full,  FRACTION_TEST)

print(f"[INFO] sizes -> train:{len(train_ds)} val:{len(val_ds)} test:{len(test_ds)}")

train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=MAX_WORKERS, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=MAX_WORKERS, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=MAX_WORKERS, pin_memory=True)
print("[OK] DataLoader ì¤€ë¹„ ì™„ë£Œ")

# -------------------------
# 6) ëª¨ë¸ í•™ìŠµ ì„¤ì •
# -------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def build_resnet50(num_classes=2):
    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
    in_features = model.fc.in_features
    model.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_features, num_classes))
    return model

model = build_resnet50().to(DEVICE)
optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)
criterion_train = nn.CrossEntropyLoss(label_smoothing=0.1)
criterion_eval  = nn.CrossEntropyLoss()
sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
scaler = torch.amp.GradScaler('cuda' if DEVICE.type=='cuda' else 'cpu')

# -------------------------
# 7) í•™ìŠµ ë£¨í”„
# -------------------------
def run_epoch(loader, train=True):
    model.train() if train else model.eval()
    loss_sum, correct, total = 0.0, 0, 0
    for xb,yb in loader:
        xb,yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)
        optimizer.zero_grad(set_to_none=True)
        use_amp = (DEVICE.type == 'cuda')
        with torch.set_grad_enabled(train), torch.amp.autocast('cuda' if use_amp else 'cpu'):
            logits = model(xb)
            loss = criterion_train(logits,yb) if train else criterion_eval(logits,yb)
        if train:
            scaler.scale(loss).backward()
            scaler.step(optimizer); scaler.update()
        pred = logits.argmax(1)
        correct += (pred==yb).sum().item()
        loss_sum += float(loss.item())*xb.size(0); total += xb.size(0)
    return loss_sum/total, correct/total

best_val = 0.0
patience_counter = 0
for ep in range(1, EPOCHS+1):
    tr_loss,tr_acc = run_epoch(train_loader, True)
    va_loss,va_acc = run_epoch(val_loader,   False)
    sched.step()
    print(f"[EP {ep:02d}] train {tr_acc:.4f} | val {va_acc:.4f}")
    if va_acc > best_val:
        best_val = va_acc
        patience_counter = 0
        torch.save(model.state_dict(), f"{META}/best_model.pth")
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping")
            break

model.load_state_dict(torch.load(f"{META}/best_model.pth", map_location=DEVICE))

# -------------------------
# 8) í‰ê°€ (ROC)
# -------------------------
@torch.no_grad()
def collect_logits(loader):
    model.eval()
    ys, ps = [], []
    for xb,yb in loader:
        xb = xb.to(DEVICE, non_blocking=True)
        use_amp = (DEVICE.type == 'cuda')
        with torch.amp.autocast('cuda' if use_amp else 'cpu'):
            prob = torch.softmax(model(xb), dim=1)[:,1].cpu().numpy()  # P(fake)
        ys.append(yb.numpy()); ps.append(prob)
    return np.concatenate(ys), np.concatenate(ps)

def plot_roc(y,p,title,path):
    fpr,tpr,_ = roc_curve(y,p)
    roc_auc   = auc(fpr,tpr)
    plt.figure()
    plt.plot(fpr,tpr,label=f"AUC={roc_auc:.4f}")
    plt.plot([0,1],[0,1],'--')
    plt.legend(); plt.title(title)
    plt.xlabel("FPR"); plt.ylabel("TPR")
    plt.savefig(path, bbox_inches='tight'); plt.close()
    return roc_auc

y_val,p_val = collect_logits(val_loader)
y_te,p_te   = collect_logits(test_loader)
auc_val     = plot_roc(y_val,p_val,"ROC(val)",f"{META}/roc_val.png")
auc_te      = plot_roc(y_te,p_te,"ROC(test)",f"{META}/roc_test.png")
print("[METRICS] AUC val/test =", round(auc_val,4), "/", round(auc_te,4))


# -------------------------
# 9) Grad-CAM + BLIP (ì˜¤ì§ Fakeë§Œ: gt=1) â€” target=fake(1)
# -------------------------
import textwrap
from transformers import BlipProcessor, BlipForConditionalGeneration

CAP_OUT = f"{META}/runs_resnet50/explanations_fake_only"
Path(CAP_OUT).mkdir(parents=True, exist_ok=True)

# Grad-CAM hooks
target_layer = model.layer4
_feat_maps, _grad_maps = [], []

def _fwd(m, x, y):
    _feat_maps.append(y.detach())

def _bwd(m, gin, gout):
    _grad_maps.append(gout[0].detach())

_fh = target_layer.register_forward_hook(_fwd)
_bh = target_layer.register_full_backward_hook(_bwd)

def grad_cam_batch_target(xb, target_class=1):
    """í•­ìƒ target_class(ê¸°ë³¸=Fake=1) ë¡œì§“ì— ëŒ€í•œ CAM ê³„ì‚°"""
    _feat_maps.clear()
    _grad_maps.clear()
    use_amp = (DEVICE.type == 'cuda')
    with torch.amp.autocast('cuda' if use_amp else 'cpu'):
        logits = model(xb)                      # [B,2]
        score  = logits[:, target_class].sum()  # Fake ë¡œì§“ í•©ì‚°
    model.zero_grad(set_to_none=True)
    score.backward()
    feats, grads = _feat_maps[-1], _grad_maps[-1]
    weights = grads.flatten(2).mean(-1).view(grads.size(0), grads.size(1), 1, 1)
    cams = torch.relu((weights * feats).sum(1))
    outs = []
    for cam in cams:
        cam_resized = TF.resize(cam.unsqueeze(0), [224,224], antialias=True)[0].detach().cpu().numpy()
        cam_norm = (cam_resized - cam_resized.min()) / (cam_resized.max() - cam_resized.min() + 1e-8)
        outs.append(cam_norm)
    return np.stack(outs)

def cam_to_overlay(pil_img, cam, alpha=0.6):
    base = np.array(pil_img.convert("RGB"), dtype=np.uint8)
    heat = (np.clip(cam, 0, 1) * 255).astype(np.uint8)
    color = cv2.applyColorMap(heat, cv2.COLORMAP_TURBO)[:, :, ::-1]
    over = cv2.addWeighted(base, 1.0, color, alpha, 0)
    return Image.fromarray(over)

def describe_cam(cam, thr=0.6):
    """
    CAMì„ ì–¼êµ´ì˜ ë‹¤ì–‘í•œ ë¶€ë¶„ìœ¼ë¡œ ì˜ê²Œ ë‚˜ëˆ ì„œ ì„¤ëª….
    - hairline / upper_forehead / eyes (left/right) / nose
    - cheeks (left/right) / mouth / chin
    """
    H, W = cam.shape
    m = cam >= thr
    if not m.any():
        return ["No strong activation region."]

    # ì„¸ë¡œ ë°©í–¥ ê¸°ì¤€ì„ 
    h1 = H // 3
    h2 = 2 * H // 3
    h_hair = int(0.2 * H)

    # ê°€ë¡œ ë°©í–¥ ê¸°ì¤€ì„ 
    w1 = W // 3
    w2 = 2 * W // 3

    regions = {
        "hairline":      m[0:h_hair, w1:w2].mean(),            # ë¨¸ë¦¬ì¹´ë½/í—¤ì–´ë¼ì¸
        "upper_forehead":m[h_hair:h1, w1:w2].mean(),           # ì´ë§ˆ ìœ„ìª½
        "left_eye":      m[h1:h2, 0:w1].mean(),
        "right_eye":     m[h1:h2, w2:W].mean(),
        "nose":          m[h1:h2, w1:w2].mean(),
        "left_cheek":    m[h2:H, 0:w1].mean(),
        "mouth":         m[h2:H, w1:w2].mean(),
        "right_cheek":   m[h2:H, w2:W].mean(),
    }

    desc = []
    for k, v in regions.items():
        if v <= 0.25:
            continue
        if k == "hairline":
            desc.append("Unnatural transitions around hairline or scalp")
        elif k == "upper_forehead":
            desc.append("Lighting inconsistency on the upper forehead region")
        elif k == "left_eye":
            desc.append("Blending or blur artifacts near the left eye area")
        elif k == "right_eye":
            desc.append("Blending or blur artifacts near the right eye area")
        elif k == "nose":
            desc.append("Plastic-like or over-smoothed texture around the nose")
        elif k == "left_cheek":
            desc.append("Texture mismatch on the left cheek region")
        elif k == "right_cheek":
            desc.append("Texture mismatch on the right cheek region")
        elif k == "mouth":
            desc.append("Sharp or inconsistent boundaries around mouth and lips")

    if not desc:
        desc = ["Localized irregular activation on facial region"]
    return desc

# BLIP (ìº¡ì…˜)
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(DEVICE).eval()

def caption_image(pil_img):
    inputs = processor(images=pil_img, return_tensors="pt").to(DEVICE)
    use_amp = (DEVICE.type == 'cuda')
    with torch.no_grad(), torch.amp.autocast('cuda' if use_amp else 'cpu'):
        out = blip.generate(**inputs, max_new_tokens=30)
    return processor.decode(out[0], skip_special_tokens=True)

def explain_fake_only(loader, split_name="val", max_total=48, also_pred_filter=False, th_fake_high=0.7):
    """
    ì •ë‹µì´ Fake(=1)ì¸ ìƒ˜í”Œë§Œ ì‹œê°í™”.
    also_pred_filter=Trueë©´ ì •ë‹µ Fake ì¤‘ì—ì„œë„ P(fake)>=th_fake_highì¸ ê³ ì‹ ë¢°ë§Œ ì„ íƒ.
    """
    records = []
    saved = 0

    mean_t = torch.tensor(DMEAN, device=DEVICE).view(3,1,1)
    std_t  = torch.tensor(DSTD,  device=DEVICE).view(3,1,1)

    for xb, yb in loader:
        mask_fake = (yb == 1)
        if mask_fake.sum().item() == 0:
            continue

        xb = xb.to(DEVICE, non_blocking=True)
        yb = yb.to(DEVICE, non_blocking=True)

        use_amp = (DEVICE.type == 'cuda')
        with torch.no_grad(), torch.amp.autocast('cuda' if use_amp else 'cpu'):
            logits = model(xb)                         # [B,2]
            probs  = torch.softmax(logits, dim=1)      # [:,1]=P(fake)

        idxs = torch.nonzero(mask_fake).squeeze(1)

        if also_pred_filter:
            idxs = idxs[probs[idxs,1] >= th_fake_high]
        if len(idxs) == 0:
            continue

        xb_sel = xb[idxs]
        cams   = grad_cam_batch_target(xb_sel, target_class=1)  # í•­ìƒ fake ë¡œì§“

        for j, k in enumerate(idxs.tolist()):
            if saved >= max_total:
                break

            p_fake = float(probs[k,1].item())
            p_real = float(probs[k,0].item())
            pred   = int(logits[k].argmax().item())

            # ì—­ì •ê·œí™” (ImageNet mean/std ê¸°ì¤€)
            img = torch.clamp(xb[k] * std_t + mean_t, 0, 1).cpu()
            pil = TF.to_pil_image(img)

            cam = cams[j]
            over = cam_to_overlay(pil, cam, alpha=0.6)
            reason  = describe_cam(cam)
            caption = caption_image(pil)

            img_path  = f"{CAP_OUT}/{split_name}_{saved:04d}.png"
            over_path = f"{CAP_OUT}/{split_name}_{saved:04d}_overlay.png"
            pil.save(img_path); over.save(over_path)

            records.append({
                "idx_global": saved,
                "gt": 1,
                "pred": pred,
                "p_real": p_real,
                "p_fake": p_fake,
                "caption": caption,
                "reason": " | ".join(reason),
            })
            saved += 1

        if saved >= max_total:
            break

    # CSV ì €ì¥
    csv_path = f"{CAP_OUT}/{split_name}_fake_only.csv"
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=["idx_global","gt","pred","p_real","p_fake","caption","reason"])
        w.writeheader()
        for r in records:
            w.writerow(r)
    print(f"[OK] {split_name} fake-only CSV:", csv_path)

        # íŒ¨ë„ ì‹œê°í™” (í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ ì•„ë˜ë¡œ)
    def _shorten(text, width=42, max_lines=2):
        """
        ê¸´ ë¬¸ì¥ì€ width ê¸€ì ë‹¨ìœ„ë¡œ ì¤„ë°”ê¿ˆí•˜ê³ ,
        max_linesë¥¼ ë„˜ìœ¼ë©´ ... ë¶™ì—¬ì„œ ì˜ë¼ì¤Œ.
        """
        if text is None:
            return ""
        # ê¸°ì¡´ ê°œí–‰/ê³µë°± ì •ë¦¬
        text = " ".join(str(text).split())
        lines = textwrap.wrap(text, width=width)
        if len(lines) > max_lines:
            lines = lines[:max_lines]
            lines[-1] += " ..."
        return "\n".join(lines)

    def show_panel(records):
        if not records:
            print("[WARN] records ë¹„ì–´ ìˆìŒ")
            return

        cols = 4   # í•„ìš”í•˜ë©´ 3ìœ¼ë¡œ ì¤„ì´ë©´ ì´ë¯¸ì§€ ë” í¼
        rows = math.ceil(len(records)/cols)

        # ì„¸ë¡œ ì‚¬ì´ì¦ˆë¥¼ ì¡°ê¸ˆ ì¤„ì—¬ì„œ ìœ„ìª½ ì—¬ë°± ì¤„ì´ê¸°
        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))
        axes = np.atleast_1d(axes).ravel()

        for i, r in enumerate(records):
            ax = axes[i]
            img = np.concatenate([
                np.array(Image.open(f"{CAP_OUT}/{split_name}_{r['idx_global']:04d}.png")),
                np.array(Image.open(f"{CAP_OUT}/{split_name}_{r['idx_global']:04d}_overlay.png"))
            ], axis=1)
            ax.imshow(img)
            ax.axis('off')

            # ìº¡ì…˜/ì´ìœ  ë„ˆë¬´ ê¸¸ë©´ ì¤„ë°”ê¿ˆ + ì˜ë¼ë‚´ê¸°
            cap_short = _shorten(r["caption"], width=42, max_lines=2)

            # reasonì€ " | " ê¸°ì¤€ìœ¼ë¡œ ì•ì—ì„œ 2ê°œê¹Œì§€ë§Œ ì‚¬ìš©
            reason_parts = str(r["reason"]).split(" | ")
            reason_join  = " | ".join(reason_parts[:2])
            reason_short = _shorten(reason_join, width=42, max_lines=2)

            header = f"Fake only | P(fake)={r['p_fake']:.2f} (pred:{r['pred']})"
            note = f"{header}\n{cap_short}\n{reason_short}"

            # y=-0.20 ìœ¼ë¡œ ë” ì•„ë˜ìª½ì— ë°°ì¹˜
            ax.text(
                0.5, -0.20,
                note,
                va='top', ha='center',
                fontsize=8, color='black',
                transform=ax.transAxes,
                bbox=dict(boxstyle='round,pad=0.3', fc=(1,1,1,0.85), ec='none')
            )

        # ë‚¨ëŠ” ì¹¸ ë¹„ìš°ê¸°
        for j in range(i+1, rows*cols):
            axes[j].axis('off')

        plt.tight_layout()
        plt.subplots_adjust(hspace=0.7)
        plt.show()

    show_panel(records)
    print(f"[OK] Grad-CAM(target=fake) + BLIP, {split_name} fake-only ì‹œê°í™” ì™„ë£Œ. ì´ {len(records)}ê°œ")

# ì‹¤í–‰: ê²€ì¦ì…‹/í…ŒìŠ¤íŠ¸ì…‹ ì¤‘ í•„ìš”ì— ë”°ë¼ í˜¸ì¶œ
explain_fake_only(val_loader,  split_name="val",  max_total=48, also_pred_filter=False)
# explain_fake_only(test_loader, split_name="test", max_total=48, also_pred_filter=False)

print("[DONE] Full pipeline (fixed mean/std + detailed facial Grad-CAM) complete.")

# ============================================
# 10) Test set ì „ì²´ BLIP ìº¡ì…˜ CSV ìƒì„±
#  - image_path, label(0/1), label_name(real/fake), blip_caption
# ============================================

import csv
from tqdm import tqdm

CAP_CSV_OUT = f"{META}/test_blip_captions.csv"

# ImageFolderë¥¼ ê·¸ëŒ€ë¡œ ì“°ë˜, transform ì—†ì´ "ê²½ë¡œ + ë¼ë²¨"ë§Œ í™œìš©
test_root = f"{DST}/test"

rows = []
for cls_name in ["real", "fake"]:
    cls_dir = os.path.join(test_root, cls_name)
    label_int = 0 if cls_name == "real" else 1
    img_paths = sorted(glob.glob(os.path.join(cls_dir, "*")))

    print(f"[INFO] {cls_name} ì´ë¯¸ì§€ {len(img_paths)}ê°œ ìº¡ì…”ë‹ ì¤‘...")
    for p in tqdm(img_paths):
        try:
            pil = Image.open(p).convert("RGB")
        except Exception as e:
            print(f"[WARN] ì´ë¯¸ì§€ ì½ê¸° ì‹¤íŒ¨: {p} ({e})")
            continue

        # BLIP ìº¡ì…˜ ìƒì„± (ìœ„ì—ì„œ ì •ì˜í•œ caption_image ì¬ì‚¬ìš©)
        caption = caption_image(pil)

        rows.append({
            "image_path": p,
            "label": label_int,
            "label_name": cls_name,
            "blip_caption": caption,
        })

# CSV ì €ì¥
with open(CAP_CSV_OUT, "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(
        f,
        fieldnames=["image_path", "label", "label_name", "blip_caption"]
    )
    writer.writeheader()
    for r in rows:
        writer.writerow(r)

print(f"[OK] Test BLIP ìº¡ì…˜ CSV ì €ì¥ ì™„ë£Œ: {CAP_CSV_OUT}")
print(f"[INFO] ì´ ìƒ˜í”Œ ìˆ˜: {len(rows)}")


# ============================================
# 11) CLIPScore ê³„ì‚° (ì´ë¯¸ì§€ vs BLIP ìº¡ì…˜)
#  - ì…ë ¥: META/test_blip_captions.csv
#  - ì¶œë ¥: META/test_blip_captions_clip.csv (clip_score ì»¬ëŸ¼ ì¶”ê°€)
# ============================================

!pip install -q ftfy regex
!pip install -q git+https://github.com/openai/CLIP.git

import clip
import csv
from tqdm import tqdm

# CLIP ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ViT-B/32)
clip_model, clip_preprocess = clip.load("ViT-B/32", device=DEVICE)

CAP_CSV_IN  = f"{META}/test_blip_captions.csv"
CAP_CSV_OUT = f"{META}/test_blip_captions_clip.csv"

rows = []
with open(CAP_CSV_IN, "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for r in reader:
        rows.append(r)

print(f"[INFO] ìº¡ì…˜ ìƒ˜í”Œ ìˆ˜: {len(rows)}")

clip_scores = []

for r in tqdm(rows):
    img_path = r["image_path"]
    caption  = r["blip_caption"]

    try:
        img = Image.open(img_path).convert("RGB")
    except Exception as e:
        print(f"[WARN] ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path} ({e})")
        r["clip_score"] = ""
        continue

    # CLIP ì „ì²˜ë¦¬
    img_tensor = clip_preprocess(img).unsqueeze(0).to(DEVICE)
    text_tokens = clip.tokenize([caption]).to(DEVICE)

    with torch.no_grad():
        img_feat  = clip_model.encode_image(img_tensor)
        text_feat = clip_model.encode_text(text_tokens)

    # ì •ê·œí™” í›„ cosine similarity
    img_feat  = img_feat / img_feat.norm(dim=-1, keepdim=True)
    text_feat = text_feat / text_feat.norm(dim=-1, keepdim=True)

    score = (img_feat @ text_feat.T).item()
    r["clip_score"] = score
    clip_scores.append(score)

# ìƒˆ CSV ì €ì¥ (clip_score í¬í•¨)
with open(CAP_CSV_OUT, "w", newline="", encoding="utf-8") as f:
    fieldnames = list(rows[0].keys())
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    for r in rows:
        writer.writerow(r)

print(f"[OK] CLIPScore ê³„ì‚° ì™„ë£Œ â†’ {CAP_CSV_OUT}")
if clip_scores:
    clip_scores = np.array(clip_scores, dtype=np.float32)
    print(f"[STATS] CLIPScore mean = {clip_scores.mean():.4f}, std = {clip_scores.std():.4f}")

    # íˆìŠ¤í† ê·¸ë¨ ì €ì¥
    plt.figure()
    plt.hist(clip_scores, bins=30)
    plt.title("CLIPScore distribution (image vs BLIP caption)")
    plt.xlabel("CLIPScore")
    plt.ylabel("Count")
    plt.savefig(f"{META}/clipscore_hist_test.png", bbox_inches="tight")
    plt.close()
    print(f"[OK] íˆìŠ¤í† ê·¸ë¨ ì €ì¥: {META}/clipscore_hist_test.png")
else:
    print("[WARN] ìœ íš¨í•œ CLIPScoreê°€ ì—†ìŒ")

# ============================================
# ResNet50 ìµœì¢… Test Accuracy / AUC ê³„ì‚°
# ============================================

import numpy as np
from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# -------------------------
# 1) í™•ë¥  ìˆ˜ì§‘ (P(fake))
# -------------------------
@torch.no_grad()
def collect_logits_resnet(loader):
    model.eval()
    ys, ps = [], []
    for xb, yb in loader:
        xb = xb.to(DEVICE, non_blocking=True)
        use_amp = (DEVICE.type == 'cuda')
        with torch.amp.autocast('cuda' if use_amp else 'cpu'):
            prob = torch.softmax(model(xb), dim=1)[:, 1].cpu().numpy()  # P(fake)
        ys.append(yb.numpy())
        ps.append(prob)
    return np.concatenate(ys), np.concatenate(ps)

# -------------------------
# 2) ë ˆì´ë¸” ì˜ˆì¸¡
# -------------------------
@torch.no_grad()
def collect_pred_resnet(loader):
    model.eval()
    all_y, all_hat = [], []
    for xb, yb in loader:
        xb = xb.to(DEVICE, non_blocking=True)
        use_amp = (DEVICE.type == 'cuda')
        with torch.amp.autocast('cuda' if use_amp else 'cpu'):
            logits = model(xb)
        preds = logits.argmax(1).cpu().numpy()
        all_y.append(yb.numpy())
        all_hat.append(preds)
    return np.concatenate(all_y), np.concatenate(all_hat)

# -------------------------
# 3) Test split í‰ê°€ ì‹¤í–‰
# -------------------------
y_true_prob, y_fake_prob = collect_logits_resnet(test_loader)
fpr, tpr, _ = roc_curve(y_true_prob, y_fake_prob)
auc_test = auc(fpr, tpr)

y_true, y_pred = collect_pred_resnet(test_loader)
acc_test = (y_true == y_pred).mean()

# -------------------------
# 4) Print Results
# -------------------------
print("=========== ResNet50 Test ì„±ëŠ¥ ===========")
print(f"Test Accuracy : {acc_test:.4f}")
print(f"Test AUC      : {auc_test:.4f}")
print("\n[Classification Report]")
print(classification_report(y_true, y_pred, target_names=['real(0)','fake(1)']))
print("[Confusion Matrix]")
print(confusion_matrix(y_true, y_pred))

# -------------------------
# 5) ROC ì €ì¥
# -------------------------
plt.figure()
plt.plot(fpr, tpr, label=f"AUC={auc_test:.4f}")
plt.plot([0,1],[0,1],'--')
plt.legend()
plt.title("ResNet50 ROC (test)")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.savefig(f"{META}/roc_test_resnet50.png", bbox_inches="tight")
plt.close()

print(f"[OK] ROC ì €ì¥ ì™„ë£Œ: {META}/roc_test_resnet50.png")
